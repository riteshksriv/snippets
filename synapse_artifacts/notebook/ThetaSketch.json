{
	"name": "ThetaSketch",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "synapse2spark",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"runAsWorkspaceSystemIdentity": false,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "b35c2767-85c1-48bb-9ddd-dd29b13e0794"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_spark",
				"display_name": "scala"
			},
			"language_info": {
				"name": "scala"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/d1370dbb-34a8-4ef2-bc9b-ed5e1107b75f/resourceGroups/iris-rg/providers/Microsoft.Synapse/workspaces/iris-synapse-ws/bigDataPools/synapse2spark",
				"name": "synapse2spark",
				"type": "Spark",
				"endpoint": "https://iris-synapse-ws.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/synapse2spark",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "2.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 15
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"\r\n",
					"import java.io.ObjectInputStream\r\n",
					"import java.io.ObjectOutputStream\r\n",
					"import java.io.IOException\r\n",
					"import java.io.Serializable\r\n",
					"import org.apache.datasketches.theta.Sketch\r\n",
					"import org.apache.datasketches.theta.UpdateSketch\r\n",
					"import org.apache.datasketches.theta.CompactSketch\r\n",
					"import org.apache.datasketches.theta.Sketches\r\n",
					"import org.apache.datasketches.memory.Memory\r\n",
					"\r\n",
					"\r\n",
					"class ThetaSketchJavaSerializable() extends Serializable {\r\n",
					"  private var sketch:Sketch = null;\r\n",
					"  def this(sketch: Sketch) {\r\n",
					"    this()\r\n",
					"    this.sketch = sketch\r\n",
					"    this.sett = 2;\r\n",
					"  }\r\n",
					"\r\n",
					"  def getSketch: Sketch = sketch\r\n",
					"\r\n",
					"  def getCompactSketch: CompactSketch = {\r\n",
					"    if (sketch == null) return null\r\n",
					"    if (sketch.isInstanceOf[UpdateSketch]) return sketch.asInstanceOf[UpdateSketch].compact\r\n",
					"    sketch.asInstanceOf[CompactSketch]\r\n",
					"  }\r\n",
					"\r\n",
					"  def update(value: String): Unit = {\r\n",
					"    if (sketch == null) sketch = UpdateSketch.builder.build\r\n",
					"    if (sketch.isInstanceOf[UpdateSketch]) sketch.asInstanceOf[UpdateSketch].update(value)\r\n",
					"    else throw new RuntimeException(\"update() on read-only sketch\")\r\n",
					"  }\r\n",
					"\r\n",
					"  def getEstimate: Double = {\r\n",
					"    if (sketch == null) return 0.0\r\n",
					"    sketch.getEstimate\r\n",
					"  }\r\n",
					"\r\n",
					"  @throws[IOException]\r\n",
					"  private def writeObject(out: ObjectOutputStream): Unit = {\r\n",
					"    if (sketch == null) {\r\n",
					"      out.writeInt(0)\r\n",
					"      return\r\n",
					"    }\r\n",
					"    val serializedSketchBytes = sketch.asInstanceOf[UpdateSketch].compact().toByteArray\r\n",
					"    out.writeInt(serializedSketchBytes.length)\r\n",
					"    out.write(serializedSketchBytes)\r\n",
					"  }\r\n",
					"\r\n",
					"  @throws[IOException]\r\n",
					"  @throws[ClassNotFoundException]\r\n",
					"  private def readObject(in: ObjectInputStream): Unit = {\r\n",
					"    val length = in.readInt\r\n",
					"    if (length == 0) return\r\n",
					"    val serializedSketchBytes = new Array[Byte](length)\r\n",
					"    in.readFully(serializedSketchBytes)\r\n",
					"    sketch = Sketches.wrapSketch(Memory.wrap(serializedSketchBytes))\r\n",
					"  }\r\n",
					"}"
				],
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"import org.apache.spark.sql.{DataFrame, Dataset, Encoder, Encoders, Row, SparkSession}\r\n",
					"import org.apache.spark.api.java.function.MapPartitionsFunction\r\n",
					"import org.apache.spark.api.java.function.ReduceFunction\r\n",
					"import org.apache.datasketches.theta.SetOperation\r\n",
					"import org.apache.datasketches.theta.UpdateSketch\r\n",
					"import org.apache.datasketches.theta.PairwiseSetOperations\r\n",
					"import java.util\r\n",
					"\r\n",
					"def computeSketch(data: DataFrame) = {\r\n",
					"    val mappedData = data.mapPartitions(new MapPartitionsFunction[Row, ThetaSketchJavaSerializable]() {\r\n",
					"      override def call(it: util.Iterator[Row]): util.Iterator[ThetaSketchJavaSerializable] = {\r\n",
					"        val sketch = new ThetaSketchJavaSerializable\r\n",
					"        while ( {\r\n",
					"          it.hasNext\r\n",
					"        }) sketch.update(it.next.get(0).asInstanceOf[String])\r\n",
					"        util.Arrays.asList(sketch).iterator\r\n",
					"      }\r\n",
					"    }, Encoders.javaSerialization(classOf[ThetaSketchJavaSerializable]))\r\n",
					"    val sketch = mappedData.reduce(new ReduceFunction[ThetaSketchJavaSerializable]() {\r\n",
					"      @throws[Exception]\r\n",
					"      override def call(sketch1: ThetaSketchJavaSerializable, sketch2: ThetaSketchJavaSerializable): ThetaSketchJavaSerializable = {\r\n",
					"        if (sketch1.getSketch == null && sketch2.getSketch == null) return new ThetaSketchJavaSerializable(UpdateSketch.builder.build.compact)\r\n",
					"        if (sketch1.getSketch == null) return sketch2\r\n",
					"        if (sketch2.getSketch == null) return sketch1\r\n",
					"        val compactSketch1 = sketch1.getCompactSketch\r\n",
					"        val compactSketch2 = sketch2.getCompactSketch\r\n",
					"        new ThetaSketchJavaSerializable(PairwiseSetOperations.union(compactSketch1, compactSketch2))\r\n",
					"      }\r\n",
					"    })\r\n",
					"    sketch\r\n",
					"  }"
				],
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"import org.apache.datasketches.theta.Sketches\r\n",
					"\r\n",
					"def intersect(sketch1: ThetaSketchJavaSerializable, sketch2: ThetaSketchJavaSerializable): Unit = {\r\n",
					"    val k = 32768;\r\n",
					"    val intersection = Sketches.setOperationBuilder.setNominalEntries(k).buildIntersection()\r\n",
					"    intersection.update(sketch1.getSketch)\r\n",
					"    intersection.update(sketch2.getSketch)\r\n",
					"    val intersectionResult = intersection.getResult\r\n",
					"    System.out.println(\"Unique intersection count: \" + intersectionResult.getEstimate)\r\n",
					"  }\r\n",
					"  def printSketchSize(sketch1: ThetaSketchJavaSerializable): Unit = {\r\n",
					"    val serializedSketchBytes = sketch1.getSketch.compact().toByteArray\r\n",
					"    System.out.println(\"sketch size\" + serializedSketchBytes.length)\r\n",
					"  }"
				],
				"execution_count": 3
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"// Read input\r\n",
					"//val streamPath = \"SM4UD805D54ISH_ORGPUIDTENANTID_0510.ss\"\r\n",
					"val streamPath = \"SM4UB5FCIPRW3O_ORGPUIDTENANTID_0630.ss\"\r\n",
					"val inputPath = \"/mnt/tmp/test/druid/\" + streamPath // streamPath\r\n",
					"val firstDf = spark.read.format(\"csv\").load(\"abfss://adlsgen2fs@irisadlsgen2.dfs.core.windows.net/synapse/workspaces/iris-synapse-ws/data/fhvhv_tripdata_2020-06.csv\")\r\n",
					"val sketch1: ThetaSketchJavaSerializable = computeSketch(firstDf)\r\n",
					"printSketchSize(sketch1)\r\n",
					"System.out.println(\"Unique count1: \" + sketch1.getEstimate)\r\n",
					"\r\n",
					"\r\n",
					"//val streamPath2 = \"SM4UDAXP91VEH2_ORGPUIDTENANTID_0350.ss\"\r\n",
					"val streamPath2 = \"SM4UD805D54ISH_ORGPUIDTENANTID_0510.ss\"\r\n",
					"val inputPath2 = \"/mnt/tmp/test/druid/\" + streamPath2 // streamPath\r\n",
					"val secondDf = spark.read.format(\"csv\").load(\"abfss://adlsgen2fs@irisadlsgen2.dfs.core.windows.net/synapse/workspaces/iris-synapse-ws/data/fhvhv_tripdata_2020-11.csv\")\r\n",
					"val sketch2: ThetaSketchJavaSerializable = computeSketch(secondDf)\r\n",
					"printSketchSize(sketch2)\r\n",
					"System.out.println(\"Unique count2: \" + sketch2.getEstimate)\r\n",
					"\r\n",
					"intersect(sketch1, sketch2)"
				],
				"execution_count": 4
			}
		]
	}
}